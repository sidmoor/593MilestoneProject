{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbe47006-83f2-4092-8b85-0e54563c3914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: 2: No such file or directory\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy<2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1650acb1-6614-4503-8ef9-efdeb1ee1e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832d5bd5-5307-470d-a7da-ce50b91097fb",
   "metadata": {},
   "source": [
    "# Code From Erik Lang\n",
    "### This section will be used as a reference for building a loop to extract the necessary information from the .HTML files. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884538d9-da76-486f-bdc8-23c742070277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your local HTML file\n",
    "# file_path = 'path/to/your/file.html'\n",
    "# Erik's comment: this 2011 example html file was saved in the same folder as this notebook, so adjust the file path accordingly\n",
    "file_path = 'Assets/NBA_Contract_Data/NBA_Contract_Extensions_2015.html'\n",
    "\n",
    "# Open and read the local file\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    content = file.read()\n",
    "\n",
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "# Find the <tbody> section\n",
    "tbody = soup.find('tbody')  #Erik's comment: the data in each year's table is between <tbody> </tbody>\n",
    "\n",
    "# If found, extract rows\n",
    "if tbody:  #Erik's comment:  \"if <tbody> </tbody> exists, perform the next operation\"\n",
    "    rows = tbody.find_all('tr')\n",
    "    for row in rows:\n",
    "        cells = row.find_all(['td', 'th'])  # include <th> just in case\n",
    "        cell_texts = [cell.get_text(strip=True) for cell in cells]\n",
    "        print(cell_texts)\n",
    "else:\n",
    "    print(\"No <tbody> found in the file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8c0557-dff4-4e86-92d6-caa733d158f4",
   "metadata": {},
   "source": [
    "# HTML File Scraper Function\n",
    "### Building a function from the provide code above\n",
    "#### This function takes a year (ex. 11,12,...20) as an input and will extract the data from the corresponding file_path. The function then takes the data and appends each list(signing year + row of individual player data) to a list. This will allow us to loop through multiple .HTML files while also creating a new column that will save the year as a new feature to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "afd4815c-3797-4066-a4d9-5ca0d72f953c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_HTML(year):\n",
    "    #Let's start with our file path. The year will be inputed so we can specify which year\n",
    "    #we want to read from before writing to our csv file\n",
    "    #The use of the year variable will also allow us to add the year as a new column to our\n",
    "    #data ensuring that we do not lose any precision when combining the 10 files into 1 CSV\n",
    "    file_path = f'Assets/NBA_Contract_Data/NBA_Contract_Extensions_20{year}.html'\n",
    "\n",
    "    #Opening and read the specified file\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "\n",
    "    #turn our read in html file into a nice pot of soup\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "    #The data we want to scrape can be found between <tbody> and </tbody>\n",
    "    #The following code finds this part (if it exists) and grabs the data from the section\n",
    "    #and will append each cell to our list of data\n",
    "    tbody = soup.find('tbody')\n",
    "    extracted_data = []\n",
    "\n",
    "    \n",
    "    if tbody:\n",
    "        rows = tbody.find_all('tr')\n",
    "        for row in rows:\n",
    "            cells = row.find_all(['td','th'])\n",
    "            cell_texts = [cell.get_text(strip=True) for cell in cells]\n",
    "            #Now we can add our year variable so we make sure we don't lose that feature\n",
    "            cell_texts.insert(0, f'20{year}')\n",
    "\n",
    "            extracted_data.append(cell_texts)\n",
    "\n",
    "    else:\n",
    "        print('No <tbody> found in the file.')\n",
    "    \n",
    "\n",
    "    return extracted_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f221cef-cc2b-43fc-983e-8afd5a8d7c72",
   "metadata": {},
   "source": [
    "# CSV Creation Function\n",
    "### Now that we have our function to scrape a specified .HTML file, we can use it to create our combined CSV file. We are going to loop through a list of years, scrape the contract data from each HTML using our above function, and save everything into a single CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8665c614-2be7-4c9a-bbaf-45ce77cef75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_contract_csv(output_file):\n",
    "    #Start with our year list and some headers we can add (and ignore later) to help with readability \n",
    "    years = [11,12,13,14,15,16,17,18,19,20]\n",
    "    headers = ['Signing Year', 'Rank', 'Player', 'Position','Team',\n",
    "               'Age', 'Contract Length', 'Total Value', 'Avg. Value',\n",
    "               'Guaranteed Money', 'Contract Type'] \n",
    "\n",
    "    #Creating an empty list that we can add each .html file data too after scraping \n",
    "    all_data = []\n",
    "    for year in years:\n",
    "        year_data = scrape_HTML(str(year))\n",
    "        #Take our list  of lists from the function and extend it to our \n",
    "        all_data.extend(year_data)\n",
    "\n",
    "    #Now that we have all of our data in 1 place we can write it to our csv file\n",
    "    with open(output_file, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "        \n",
    "        #create our writer object that will allow us to add to our output_file\n",
    "        writer = csv.writer(csv_file)\n",
    "        #start with adding our header information to the file\n",
    "        writer.writerow(headers)\n",
    "\n",
    "        for row in all_data:\n",
    "            writer.writerow(row)\n",
    "\n",
    "    print(f'Your CSV file {output_file} has been created and saved to the Assets Folder')\n",
    "    #By default this function will save the CSV file to the same folder that this notebook\n",
    "    #was run in unless we specify the Assets folder in out output_file variable\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "830b430d-cfe4-4e3d-afdf-79de54b90b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your CSV file Assets/NBA_Contracts.csv has been created and saved to the Assets Folder\n"
     ]
    }
   ],
   "source": [
    "#Now lets create our output_file path name and call our function\n",
    "output_file_path = 'Assets/NBA_Contracts.csv'\n",
    "\n",
    "create_contract_csv(output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd601f6-e046-46ad-9ab0-04d079776dff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
